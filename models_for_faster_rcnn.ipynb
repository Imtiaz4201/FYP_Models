{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import json\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from object_detection.utils import visualization_utils\n",
    "from object_detection.builders import model_builder\n",
    "import xml.etree.ElementTree as ET\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download tensorflow 2 object detection api\n",
    "https://github.com/tensorflow/models/tree/master/research/object_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set XLA_FLAGS=--xla_gpu_cuda_data_dir=C:\\Users\\Imtiaz\\anaconda3\\envs\\tf_gpu\\Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd C:\\Users\\Imtiaz\\Downloads\\DL_models\\models\\research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set PIPELINE_CONFIG_PATH=C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\faster_rcnn_data\\\\pre_trained_model_resnet50_640_by_640\\\\faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.config\n",
    "set MODEL_DIR=C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\faster_rcnn_data\\\\train_events\\\\resnet50_evnt1\n",
    "\n",
    "python object_detection/model_main_tf2.py ^\n",
    "    --pipeline_config_path=%PIPELINE_CONFIG_PATH% ^\n",
    "    --model_dir=%MODEL_DIR% ^\n",
    "    --alsologtostderr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set PIPELINE_CONFIG_PATH=C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\faster_rcnn_data\\\\pre_trained_model_resnet50_640_by_640\\\\faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.config\n",
    "set MODEL_DIR=C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\faster_rcnn_data\\\\train_events\\\\resnet50_evnt1\n",
    "set CHECKPOINT_DIR=C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\faster_rcnn_data\\\\train_events\\\\resnet50_evnt1\n",
    "\n",
    "\n",
    "python object_detection/model_main_tf2.py ^\n",
    "    --pipeline_config_path=%PIPELINE_CONFIG_PATH% ^\n",
    "    --model_dir=%MODEL_DIR% ^\n",
    "    --checkpoint_dir=%CHECKPOINT_DIR% ^\n",
    "    --alsologtostderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_paths = []\n",
    "for root, dirs, files in os.walk('data\\\\images\\\\test'):\n",
    "    for i in files:\n",
    "        test_image_paths.append(\n",
    "            os.path.join(os.getcwd(),root,i)\n",
    "        )\n",
    "test_image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd C:\\Users\\Imtiaz\\Downloads\\DL_models\\models\\research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\saved_models\\\\en\\\\pipeline.config\"\n",
    "config = config_util.get_configs_from_pipeline_file(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = model_builder.build(model_config=config['model'], is_training=False)\n",
    "# Restore checkpoint\n",
    "checkpoint_path = \"C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\faster_rcnn_data\\\\train_events\\\\evn8\"\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(checkpoint_path, 'ckpt-3')).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_path_ = \"C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\annotations\\\\labelmap.pbtxt\"\n",
    "class_labels = label_map_util.create_category_index_from_labelmap(label_map_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def predict(input_image):\n",
    "    img = cv2.imread(input_image)\n",
    "    input_tensor = tf.convert_to_tensor(\n",
    "        np.expand_dims(img, 0), dtype=tf.float32)\n",
    "    image, shapes = detection_model.preprocess(input_tensor)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "def detect(input_image):\n",
    "    @tf.function\n",
    "    def process(img):\n",
    "        image, shapes = detection_model.preprocess(img)\n",
    "        prediction_dict = detection_model.predict(image, shapes)\n",
    "        detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "        return detections\n",
    "\n",
    "    img = cv2.imread(input_image)\n",
    "    input_tensor = tf.convert_to_tensor(\n",
    "        np.expand_dims(img, 0), dtype=tf.float32)\n",
    "\n",
    "    detections = process(input_tensor)\n",
    "\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(\n",
    "        np.int64)\n",
    "    label_id_offset = 1\n",
    "    image_with_detections = img.copy()\n",
    "    visualization_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_with_detections,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes']+label_id_offset,\n",
    "        detections['detection_scores'],\n",
    "        class_labels,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=5,\n",
    "        min_score_thresh=.8,\n",
    "        agnostic_mode=False)\n",
    "\n",
    "    return image_with_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_image_paths)):\n",
    "    detection_image = detect(test_image_paths[i])\n",
    "    cv2.imwrite(f\"pre_img\\\\pred1\\\\pre{i+1}.jpg\", detection_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\saved_models\\\\newFRCNNRESNET101EXP4\\\\saved_model\"\n",
    "loaded_model =  tf.saved_model.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(\"C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\annotations\\\\labelmap.pbtxt\",\n",
    "                                                                    use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for root, dirs, files in os.walk(os.path.join(os.getcwd(),'NE')):\n",
    "    for i in files:\n",
    "        image_paths.append(\n",
    "            os.path.join(os.getcwd(),root,i)\n",
    "        )\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.CE_modified import ContastEnhancement\n",
    "\n",
    "def inference(path):\n",
    "    try:\n",
    "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "        # ce = ContastEnhancement(img)\n",
    "        # Pce = ce.prcoess_image()\n",
    "        image_np = np.array(img)\n",
    "        input_tensor = tf.convert_to_tensor(image_np)\n",
    "        input_tensor = input_tensor[tf.newaxis, ...]\n",
    "        detections = loaded_model(input_tensor)\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy()\n",
    "                      for key, value in detections.items()}\n",
    "        detections['num_detections'] = num_detections\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "        image_np_with_detections = image_np.copy()\n",
    "\n",
    "        # Check if class name is 'other' and change it to 'light pole'\n",
    "        if category_index[detections['detection_classes'][0]]['name'] == 'other':\n",
    "            category_index[detections['detection_classes'][0]]['name'] = 'light pole'\n",
    "\n",
    "        # Visualize detections\n",
    "        visualization_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np_with_detections,\n",
    "            detections['detection_boxes'],\n",
    "            detections['detection_classes'],\n",
    "            detections['detection_scores'],\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=5,\n",
    "            min_score_thresh=0.8,\n",
    "            agnostic_mode=False\n",
    "        )\n",
    "\n",
    "        return image_np_with_detections\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "# ... other imports\n",
    "\n",
    "start_time = time.time()  # Record the start time before the loop\n",
    "\n",
    "for i in range(len(image_paths)):\n",
    "    name = re.findall(re.compile(r\"[a-zA-Z-0-9\\_]+\"), image_paths[i])[-2]\n",
    "    di = inference(image_paths[i])\n",
    "    cv2.imwrite(f\"results\\\\faster rcnn101NE\\\\{name}.png\", di)\n",
    "\n",
    "end_time = time.time()  # Record the end time after the loop\n",
    "total_execution_time = end_time - start_time  # Calculate the total execution time\n",
    "\n",
    "print(f\"Total execution time for processing {len(image_paths)} images: {total_execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training localization and classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_box_loss_cv = os.path.join(os.getcwd(\n",
    "), \"saved_models\\\\faster_rcnn_resnet101\\\\en\\\\train_loc_loss.csv\")\n",
    "train_box_loss_cv = pd.read_csv(train_box_loss_cv)\n",
    "train_class_loss_cv = os.path.join(os.getcwd(\n",
    "), \"saved_models\\\\faster_rcnn_resnet101\\\\en\\\\train_class_loss.csv\")\n",
    "train_class_loss_cv = pd.read_csv(train_class_loss_cv)\n",
    "train_total_loss = os.path.join(os.getcwd(\n",
    "), \"saved_models\\\\faster_rcnn_resnet101\\\\en\\\\train_total_loss.csv\")\n",
    "train_total_loss = pd.read_csv(train_total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"Step\", y=\"Value\", data=train_box_loss_cv)\n",
    "plt.title(\"Training localization loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"Step\", y=\"Value\", data=train_class_loss_cv)\n",
    "plt.title(\"Training classification loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"Step\", y=\"Value\", data=train_total_loss)\n",
    "plt.title(\"Training Total loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation localization and classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_box_loss_cv = os.path.join(os.getcwd(\n",
    "), \"saved_models\\\\faster_rcnn_resnet101\\\\en\\\\val_loc_loss.csv\")\n",
    "val_box_loss_cv = pd.read_csv(val_box_loss_cv)\n",
    "val_class_loss_cv = os.path.join(os.getcwd(\n",
    "), \"saved_models\\\\faster_rcnn_resnet101\\\\en\\\\val_class_loss.csv\")\n",
    "val_class_loss_cv = pd.read_csv(val_class_loss_cv)\n",
    "val_total_loss =  os.path.join(os.getcwd(\n",
    "), \"saved_models\\\\faster_rcnn_resnet101\\\\en\\\\val_total_loss.csv\")\n",
    "val_total_loss = pd.read_csv(val_total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"Step\", y=\"Value\", data=val_box_loss_cv)\n",
    "plt.title(\"Validation localization loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"Step\", y=\"Value\", data=val_class_loss_cv)\n",
    "plt.title(\"Validation classification loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"Step\", y=\"Value\", data=val_total_loss)\n",
    "plt.title(\"Validation Total loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix, precision , recall and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"tm\": 1,\n",
    "    \"tnb\": 2,\n",
    "    \"other\": 3\n",
    "}\n",
    "filename = os.path.join(os.getcwd(), \"annotations\", \"labels_dict.json\")\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(labels, json_file)\n",
    "print(\"Labels have been written to\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def save_to_csv(data):\n",
    "    counter = 0\n",
    "    image_pred = pd.DataFrame()\n",
    "    inference_time = {\"image\":[], \"time\":[]}\n",
    "    try:\n",
    "        for image in data:\n",
    "            print('Running inference for {}... '.format(image.split('/')[-1]), end='')\n",
    "            start_time = time.time()  # Record start time\n",
    "            img = cv2.imread(image, cv2.IMREAD_UNCHANGED)\n",
    "            image_np = np.array(img)\n",
    "            height, width, l = image_np.shape\n",
    "            input_tensor = tf.convert_to_tensor(image_np)\n",
    "            input_tensor = input_tensor[tf.newaxis, ...]\n",
    "            detections = loaded_model(input_tensor)\n",
    "            num_detections = int(detections.pop('num_detections'))\n",
    "            detections = {key: value[0, :num_detections].numpy()\n",
    "                          for key, value in detections.items()}\n",
    "            \n",
    "            for i in range(num_detections):\n",
    "                ll = []\n",
    "                ll.append(image)\n",
    "                ll.append(width)\n",
    "                ll.append(height)\n",
    "                ll.append(detections['detection_boxes'][i].tolist())\n",
    "                ll.append(category_index[detections['detection_classes'][i]]['name'])\n",
    "                ll.append(detections['detection_scores'][i])\n",
    "            \n",
    "                pre_df = pd.DataFrame([{'filename': ll[0],\n",
    "                                        'width': ll[1],\n",
    "                                        'height': ll[2],\n",
    "                                        'class': ll[4],\n",
    "                                        'xmin': ll[3][1] * width,\n",
    "                                        'ymin': ll[3][0] * height,\n",
    "                                        'xmax': ll[3][3] * width,\n",
    "                                        'ymax': ll[3][2] * height,\n",
    "                                        'score': ll[5]}])\n",
    "                image_pred = pd.concat([image_pred, pre_df], axis=0)\n",
    "            \n",
    "            end_time = time.time()  # Record end time\n",
    "            inference_time['image'].append(image)\n",
    "            inference_time['time'].append(end_time - start_time)  # Calculate inference time\n",
    "            counter += 1\n",
    "            print(f'Done : {counter}')\n",
    "        return image_pred, inference_time\n",
    "    except Exception as e:\n",
    "        print(\"image has issue :\" + image.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data, pred_time = save_to_csv(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_path = os.path.join(\n",
    "    os.getcwd(), \"ssd_data\\\\train_events\\\\evn1\", \"pred_data.csv\")\n",
    "pred_time_path = os.path.join(\n",
    "    os.getcwd(), \"ssd_data\\\\train_events\\\\evn1\", \"pred_time.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data.to_csv(pred_data_path, index=False)\n",
    "pd.DataFrame(pred_time).to_csv(pred_time_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_csv = pd.read_csv(\n",
    "    \"dataset_csv\\\\test_data.csv\"\n",
    ")\n",
    "pred_data_read = pd.read_csv(pred_data_path)\n",
    "pred_time_read = pd.read_csv(pred_time_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(groundtruth_box, detection_box):\n",
    "    g_xmin, g_ymin, g_xmax, g_ymax = tuple(groundtruth_box)\n",
    "    d_xmin, d_ymin, d_xmax, d_ymax = tuple(detection_box)\n",
    "    xa = max(g_xmin, d_xmin)\n",
    "    ya = max(g_ymin, d_ymin)\n",
    "    xb = min(g_xmax, d_xmax)\n",
    "    yb = min(g_ymax, d_ymax)\n",
    "    intersection = max(0, xb - xa + 1) * max(0, yb - ya + 1)\n",
    "    boxAArea = (g_xmax - g_xmin + 1) * (g_ymax - g_ymin + 1)\n",
    "    boxBArea = (d_xmax - d_xmin + 1) * (d_ymax - d_ymin + 1)\n",
    "    return intersection / float(boxAArea + boxBArea - intersection)\n",
    "\n",
    "\n",
    "def process_detections(ground_truth_csv, pred_csv, IOU_THRESHOLD, CONFIDENCE_THRESHOLD, categories):\n",
    "    confusion_matrix = np.zeros(\n",
    "        shape=(len(categories) + 1, len(categories) + 1))\n",
    "    file_unique = ground_truth_csv['filename'].unique()\n",
    "    for file in file_unique:\n",
    "        test_df = ground_truth_csv[ground_truth_csv['filename'] == file]\n",
    "        test_df.reset_index(inplace=True, drop=True)\n",
    "        pred_df = pred_csv[pred_csv['filename'] == file]\n",
    "        pred_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        pred_class = pred_df[pred_df['score'] >= CONFIDENCE_THRESHOLD]\n",
    "\n",
    "        groundtruth_boxes = test_df[[\n",
    "            'xmin', 'ymin', 'xmax', 'ymax']].values.tolist()\n",
    "        detection_boxes = pred_class[[\n",
    "            'xmin', 'ymin', 'xmax', 'ymax']].values.tolist()\n",
    "        #print(groundtruth_boxes)\n",
    "        #print(detection_boxes)\n",
    "        matches = []\n",
    "\n",
    "        for i in range(len(groundtruth_boxes)):\n",
    "            for j in range(len(detection_boxes)):\n",
    "                iou = compute_iou(groundtruth_boxes[i], detection_boxes[j])\n",
    "\n",
    "                if iou > IOU_THRESHOLD:\n",
    "                    matches.append([i, j, iou])\n",
    "\n",
    "        matches = np.array(matches)\n",
    "\n",
    "        #print(matches)\n",
    "\n",
    "        if matches.shape[0] > 0:\n",
    "            # Sort list of matches by descending IOU so we can remove duplicate detections\n",
    "            # while keeping the highest IOU entry.\n",
    "            matches = matches[matches[:, 2].argsort()[::-1][:len(matches)]]\n",
    "\n",
    "            # Remove duplicate detections from the list.\n",
    "            matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "\n",
    "            # Sort the list again by descending IOU. Removing duplicates doesn't preserve\n",
    "            # our previous sort.\n",
    "            matches = matches[matches[:, 2].argsort()[::-1][:len(matches)]]\n",
    "\n",
    "            # Remove duplicate ground truths from the list.\n",
    "            matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "\n",
    "        for i in range(len(groundtruth_boxes)):\n",
    "            #print(i)\n",
    "            if matches.shape[0] > 0 and matches[matches[:, 0] == i].shape[0] == 1:\n",
    "                #print(\"inside : \", i)\n",
    "                confusion_matrix[categories[test_df['class'][i]] - 1][categories[pred_class['class']\n",
    "                                                                                 [matches[matches[:, 0] == i].tolist()[0][1]]] - 1] += 1\n",
    "            else:\n",
    "                confusion_matrix[categories[test_df['class']\n",
    "                                            [i]] - 1][confusion_matrix.shape[1] - 1] += 1\n",
    "\n",
    "        for i in range(len(detection_boxes)):\n",
    "            if matches.shape[0] > 0 and matches[matches[:, 1] == i].shape[0] == 0:\n",
    "                confusion_matrix[confusion_matrix.shape[0] -\n",
    "                                 1][categories[pred_class['class'][i]] - 1] += 1\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def create_CM(ground_truth_csv, pred_csv, IOU_THRESHOLD=0.3, CONFIDENCE_THRESHOLD=0.3):\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    ground_truth_csv['filename'] = ground_truth_csv['filename'].apply(\n",
    "        lambda x: x.split('\\\\')[-1])\n",
    "\n",
    "    pred_csv['filename'] = pred_csv['filename'].apply(\n",
    "        lambda x: x.split('\\\\')[-1])\n",
    "    label_map = \"C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\annotations\\\\labels_dict.json\"\n",
    "    output_path = \"confusion_matrix.csv\"\n",
    "    with open(label_map, \"r\") as json_file:\n",
    "        categories = json.load(json_file)\n",
    "\n",
    "    results = []\n",
    "    class_uniq = pred_csv['class'].unique()\n",
    "    confusion_matrix = process_detections(\n",
    "        ground_truth_csv, pred_csv, IOU_THRESHOLD, CONFIDENCE_THRESHOLD, categories)\n",
    "    for label in class_uniq:\n",
    "        class_id = int(float(categories[label]))-1\n",
    "        name = label\n",
    "\n",
    "        total_target = np.sum(confusion_matrix[class_id, :])\n",
    "        total_predicted = np.sum(confusion_matrix[:, class_id])\n",
    "\n",
    "        # precision = float(\n",
    "        #     confusion_matrix[class_id, class_id] / total_predicted)\n",
    "        # recall = float(confusion_matrix[class_id, class_id] / total_target)\n",
    "        try:\n",
    "            precision = float(confusion_matrix[class_id, class_id] / total_predicted)\n",
    "            if math.isnan(precision):\n",
    "                precision = 0.0\n",
    "        except Exception:\n",
    "            precision = 0.0\n",
    "\n",
    "        try:\n",
    "            recall = float(confusion_matrix[class_id, class_id] / total_target)\n",
    "            if math.isnan(recall):\n",
    "                recall = 0.0\n",
    "        except Exception:\n",
    "            recall = 0.0\n",
    "\n",
    "        results.append({'category': name, 'precision_@{}IOU'.format(IOU_THRESHOLD): precision, 'recall_@{}IOU'.format(IOU_THRESHOLD): recall})\n",
    "    print(confusion_matrix)\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df)\n",
    "    df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_CM(ground_truth_csv, pred_data_read,IOU_THRESHOLD=0.1,CONFIDENCE_THRESHOLD=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "command = ['python', 'confusion_matrix_tf2.py',\n",
    "           '--input_tfrecord_path', 'C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\annotations\\\\test_new.record',\n",
    "           '--output_path', 'C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\metrics',\n",
    "           '--inference_graph', 'C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\saved_models\\\\newFRCNNRESNET50EXP1\\\\saved_model',\n",
    "           '--class_labels', 'C:\\\\Users\\\\Imtiaz\\\\Downloads\\\\DL_models\\\\annotations\\\\labelmap.pbtxt']\n",
    "\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(\"Execution time:\", execution_time, \"seconds\")\n",
    "print(\"Output:\", result.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python confusion_matrix_tf2.py --input_tfrecord_path xx --output_path xx --inference_graph xx --class_labels xx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
